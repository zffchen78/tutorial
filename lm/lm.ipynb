{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%pylab inline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language Modeling Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Task : Given a sequence of words, predict the next word\n",
    "  - Models the probability of sentences in a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sequence\n",
    "$$ \n",
    "\\begin{eqnarray}\n",
    "x & = & x_1, x_2, x_3, ..., x_n \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "* E.g.,\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "x & = & 明月几时有\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "$$\n",
    "x_1 = 明, x_2 = 月, x_3 = 几, x_4 = 时, x_5 = 有\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Handle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "    \n",
    "    def __init__(self, corpus, batch, steps):\n",
    "        self.batch = batch\n",
    "        self.steps = steps\n",
    "        words = open(corpus, mode='r').read().replace('\\n', '_')\n",
    "        words_as_set = set(words)\n",
    "        self.id_to_word = sorted(set(words))\n",
    "        self.word_to_id = {w: i for i, w in enumerate(self.id_to_word)}\n",
    "        self.data = [self.word_to_id[w] for w in words]\n",
    "        print('Number of unique chars: ', len(self.id_to_word))\n",
    "        print('Number of training chars: ', len(self.data))\n",
    "        self.seqgen = self.seq_generator()\n",
    "\n",
    "    def seq_generator(self):\n",
    "        curr = 0\n",
    "        while True:\n",
    "            if curr > len(self.data) - self.steps - 1:\n",
    "                curr = 0\n",
    "            start, limit = curr, curr + self.steps\n",
    "            w, t = (self.data[start:limit], self.data[start + 1:limit + 1])\n",
    "            curr = limit\n",
    "            yield w, t\n",
    "\n",
    "    def get_batch(self):\n",
    "        input, target = [], []\n",
    "        for _ in range(self.batch):\n",
    "            w, t = next(self.seqgen)\n",
    "            input.append(w)\n",
    "            target.append(t)\n",
    "        return np.array(input), np.array(target)\n",
    "    \n",
    "    def to_words(self, ids):\n",
    "        return [self.id_to_word[x] for x in ids]\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return len(self.id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class TrainData(object):\n",
    "    \n",
    "    def __init__(self, corpus, batch, steps):\n",
    "        self.batch = batch\n",
    "        self.steps = steps\n",
    "        words = open(corpus, mode='r').read().replace('\\n', '_')\n",
    "        words_as_set = set(words)\n",
    "        self.id_to_word = sorted(set(words))\n",
    "        self.word_to_id = {w: i for i, w in enumerate(self.id_to_word)}\n",
    "        self.data = [self.word_to_id[w] for w in words]\n",
    "        tf.logging.info('Number of unique chars: ', len(self.id_to_word))\n",
    "        tf.logging.info('Number of training chars: ', len(self.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "    def __init__(...):\n",
    "      self.seqgen = self.seq_generator()        \n",
    "\n",
    "    def seq_generator(self):\n",
    "        curr = 0\n",
    "        while True:\n",
    "            if curr > len(self.data) - self.steps - 1:\n",
    "                curr = 0\n",
    "            start, limit = curr, curr + self.steps\n",
    "            w, t = (self.data[start:limit], self.data[start + 1:limit + 1])\n",
    "            curr = limit\n",
    "            yield w, t\n",
    "\n",
    "    def get_batch(self):\n",
    "        input, target = [], []\n",
    "        for _ in range(self.batch):\n",
    "            w, t = next(self.seqgen)\n",
    "            input.append(w)\n",
    "            target.append(t)\n",
    "        return np.array(input), np.array(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    def to_words(self, ids):\n",
    "        return [self.id_to_word[x] for x in ids]\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return len(self.id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars:  7957\n",
      "Number of training chars:  4888154\n",
      "['秦', '川', '雄', '帝', '宅', '，', '函', '谷', '壮', '皇']\n",
      "['川', '雄', '帝', '宅', '，', '函', '谷', '壮', '皇', '居']\n"
     ]
    }
   ],
   "source": [
    "data = TrainData('./data/poem.txt', 1, 10)\n",
    "x, y = data.get_batch()\n",
    "print(data.to_words(x[0]))\n",
    "print(data.to_words(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A bit of maths\n",
    "\n",
    "* Model: the probability of a sequence\n",
    "$$ p_\\theta(x) = p_\\theta(x_1)p_\\theta(x_2|x_1)p_\\theta(x_3|x_1x_2)...p_\\theta(x_n|x_1x_2...x_{n-1}) $$\n",
    "* $\\theta$ to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Maximum likelihood estimation.\n",
    "$$ \n",
    "\\operatorname*{arg\\,max}_\\theta \\prod_{x\\in D} p_\\theta(x)\n",
    "$$\n",
    "\n",
    "* Equivalent to\n",
    "$$ \n",
    " -\\frac{1}{N}\\operatorname*{arg\\,min}_\\theta \\sum_{x\\in D} log(p_\\theta(x))\n",
    " = -\\frac{1}{N} \\operatorname*{arg\\,min}_\\theta \\sum_{x\\in D} \\sum_i log(p_\\theta(x_i|x_1x_2...x_{i-1}))\n",
    "$$\n",
    "\n",
    "  $D$ is the data set and $N$ is the number of samples in the data set.\n",
    "\n",
    "* Per-word loss term:\n",
    "$$\n",
    "-log(p_\\theta(x_i|x_1x_2...x_{i-1}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's build the following model\n",
    "  - Character embedding\n",
    "  - A recurrent neural network\n",
    "  - Stacked, unrolled in time\n",
    "  - Long short term memory (LSTM) cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='data/lstm.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM Cell\n",
    "  - Takes input, previous output and current state, and produces output and next state.\n",
    "  \n",
    "$$\n",
    "h_t, C_t = lstm(x_t, h_{t-1}, C_{t-1})\n",
    "$$\n",
    "\n",
    "<img align='center' src='data/lstm_cell.png' width='40%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Full set of equations ($[]$ is vector concatenation, $\\times$ is matrix multiply, $*$ is element-wise multiply)\n",
    "\n",
    "$$ X = [h_{t-1}, x_t] $$\n",
    "$$ f_t = \\sigma(W_f \\times X + b_f) $$\n",
    "$$ i_t = \\sigma(W_i \\times X + b_i) $$\n",
    "$$ o_t = \\sigma(W_o \\times X + b_o) $$\n",
    "$$ \\tilde{C}_t = tanh(W_C \\times X + b_C) $$\n",
    "$$ C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "$$ h_t = o_t * tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Implement an LSTM cell as a class, so we can instantiate many layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    \n",
    "    def __init__(self, ith, dims):\n",
    "        self.dims = dims\n",
    "        with tf.name_scope('lstm_%d' % ith):\n",
    "            self.W_f = tf.Variable(self.initializer(), name='wf')\n",
    "            self.W_i = tf.Variable(self.initializer(), name='wi')\n",
    "            self.W_o = tf.Variable(self.initializer(), name='wo')\n",
    "            self.W_C = tf.Variable(self.initializer(), name='wc')\n",
    "            self.b_f = tf.Variable(tf.zeros([dims]), name='bf')\n",
    "            self.b_i = tf.Variable(tf.zeros([dims]), name='bi')\n",
    "            self.b_o = tf.Variable(tf.zeros([dims]), name='bo')\n",
    "            self.b_C = tf.Variable(tf.zeros([dims]), name='bc')\n",
    "\n",
    "    def forward(self, x_t, h_t1, C_t1):\n",
    "        X = tf.concat(1, [h_t1, x_t])\n",
    "        f_t = tf.sigmoid(tf.matmul(X, self.W_f) + self.b_f)\n",
    "        i_t = tf.sigmoid(tf.matmul(X, self.W_i) + self.b_i)\n",
    "        o_t = tf.sigmoid(tf.matmul(X, self.W_o) + self.b_o)\n",
    "        Ctilde_t = tf.tanh(tf.matmul(X, self.W_C) + self.b_C)\n",
    "        C_t = f_t * C_t1 + i_t * Ctilde_t\n",
    "        h_t = o_t * tf.tanh(C_t)\n",
    "        return h_t, C_t\n",
    "\n",
    "    def initializer(self):\n",
    "        return tf.random_uniform([2*self.dims, self.dims], -0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's build the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, dims, vocab, depth, steps, lr):\n",
    "        # Configs.\n",
    "        self.dims = dims\n",
    "        self.vocab = vocab\n",
    "        self.depth = depth\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Var\n",
    "            self.embedding = tf.Variable(\n",
    "                tf.random_uniform([vocab, dims], -0.02, 0.02))\n",
    "            self.lstm = []\n",
    "            for i in range(depth):\n",
    "                self.lstm.append(LSTM(i, self.dims))\n",
    "            with tf.name_scope('sm'):\n",
    "                self.sm_w = tf.Variable(\n",
    "                    tf.random_uniform([dims, vocab], -0.1, 0.1),\n",
    "                    name='w')\n",
    "                self.sm_b = tf.Variable(\n",
    "                    tf.zeros([vocab]), name='b')\n",
    "\n",
    "            # Feeds.\n",
    "            self.words = tf.placeholder(tf.int64)\n",
    "            self.targets = tf.placeholder(tf.int64)\n",
    "        \n",
    "            # Define forward.\n",
    "            batch_size = tf.shape(self.words)[:1] \n",
    "            shape = tf.concat(0, [batch_size, [dims]]) \n",
    "            init_zeros = tf.zeros(shape)\n",
    "            h = [init_zeros] * depth\n",
    "            c = [init_zeros] * depth\n",
    "            o = []\n",
    "            \n",
    "            # Unroll LSTMs.\n",
    "            for i in range(steps):\n",
    "                # Get the embedding for words\n",
    "                x = tf.nn.embedding_lookup(\n",
    "                    self.embedding, self.words[:, i])\n",
    "                for j in range(self.depth):\n",
    "                    h[j], c[j] = self.lstm[j].forward(x, h[j], c[j])\n",
    "                    x = h[j]\n",
    "                o.append(x)\n",
    "            outputs = tf.reshape(tf.concat(1, o), [-1, dims])\n",
    "            logits = tf.matmul(outputs, self.sm_w) + self.sm_b\n",
    "                \n",
    "            # Compute the loss.\n",
    "            costs = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits, tf.reshape(self.targets, [-1]))\n",
    "            self.loss = tf.reduce_mean(costs)\n",
    "            \n",
    "            # Define gradients, optimizer.\n",
    "            self.global_step = tf.Variable(\n",
    "                0, trainable=False, name='global_step')\n",
    "            vars = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.loss, vars)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            self.train_op = optimizer.apply_gradients(\n",
    "                zip(grads, vars), global_step=self.global_step)\n",
    "\n",
    "            # Summary\n",
    "            tf.scalar_summary('loss', self.loss)\n",
    "            self.summary = tf.merge_summary(\n",
    "                tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "            \n",
    "            # Saver\n",
    "            self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "            # Init\n",
    "            init = tf.initialize_all_variables()\n",
    "\n",
    "            # Inference\n",
    "            self.in_h = []\n",
    "            self.in_c = []\n",
    "            self.out_h = []\n",
    "            self.out_c = []\n",
    "            x = tf.nn.embedding_lookup(self.embedding, self.words)\n",
    "            for i in range(self.depth):\n",
    "                h = tf.placeholder(tf.float32)\n",
    "                c = tf.placeholder(tf.float32)\n",
    "                self.in_h.append(h)\n",
    "                self.in_c.append(c)\n",
    "                h, c = self.lstm[i].forward(x, h, c)\n",
    "                self.out_h.append(h)\n",
    "                self.out_c.append(c)\n",
    "                x = h\n",
    "            logits = tf.matmul(x, self.sm_w) + self.sm_b\n",
    "            self.preds = tf.nn.softmax(logits)\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(init)  \n",
    "            \n",
    "    def train(self, data, logdir, total_steps):\n",
    "        swriter = tf.train.SummaryWriter(logdir)\n",
    "        \n",
    "        # Recover.\n",
    "        latest = tf.train.latest_checkpoint(logdir)\n",
    "        if latest is not None:\n",
    "            print('restore ', latest)\n",
    "            self.saver.restore(self.sess, latest)\n",
    "\n",
    "        steps = self.sess.run(self.global_step)\n",
    "        while steps < total_steps:\n",
    "            if steps % 1000 == 0:\n",
    "                self.saver.save(\n",
    "                    self.sess, logdir + '/lm_params', global_step=steps)\n",
    "                w, t = data.get_batch()\n",
    "            if steps % 100 == 0:\n",
    "                loss, summary = self.sess.run(\n",
    "                    [self.loss, self.summary],\n",
    "                    feed_dict={self.words: w, self.targets: t})\n",
    "                swriter.add_summary(summary, steps)\n",
    "                swriter.flush()\n",
    "                print('step %d: %.3f' % (steps, loss))\n",
    "            else:\n",
    "                self.sess.run(\n",
    "                    self.train_op,\n",
    "                    feed_dict={self.words: w, self.targets: t})\n",
    "            steps += 1\n",
    "            \n",
    "    def load(self, checkpoint):\n",
    "        self.saver.restore(self.sess, checkpoint)\n",
    "        \n",
    "    def inference(self, data, prefix, num):\n",
    "        feeds = {}\n",
    "        zeros = np.zeros([1, self.dims])\n",
    "        for i in range(self.depth):\n",
    "            feeds[self.in_h[i]] = zeros\n",
    "            feeds[self.in_c[i]] = zeros\n",
    "        feeds[self.words] = np.zeros([1], dtype=np.int64)\n",
    "        output = []\n",
    "        for i in range(len(prefix) + num):\n",
    "            if i < len(prefix):\n",
    "                id = data.word_to_id[prefix[i]]\n",
    "            else:\n",
    "                id = int(np.argwhere(np.cumsum(probs) >= np.random.rand())[0])\n",
    "                output.append(id)\n",
    "            feeds[self.words][0] = id\n",
    "            vals = self.sess.run(self.out_h + self.out_c + [self.preds],\n",
    "                                 feed_dict=feeds)\n",
    "            for i in range(self.depth):\n",
    "                feeds[self.in_h[i]] = vals[i]\n",
    "                feeds[self.in_c[i]] = vals[self.depth + i]\n",
    "            probs = np.reshape(vals[-1], [-1])\n",
    "        return prefix + ''.join([data.id_to_word[x] for x in output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters of the model\n",
    "* We need to pick embedding dimensions and the dimensions of the state vector.\n",
    "  - For convenience, let's pick `dims = 256`\n",
    "* Vocab size.\n",
    "  - `data.vocab = 7957`\n",
    "* Embedding vectors\n",
    "  - `[7957, dims]`.\n",
    "* The 4 weight matrices in the equation ($W_f, W_i, W_o, W_C$)\n",
    "  - `[2 * dims, dims]`\n",
    "* 4 biases ($b_f, b_i, b_o, b_C$)\n",
    "  - `[dims]`\n",
    "* Softmax classifier logit layer weights and biases\n",
    "  - `[dims, 7957], [7957]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, dims, vocab, depth, steps, lr):\n",
    "        # Configs.\n",
    "        self.dims = dims\n",
    "        self.vocab = vocab\n",
    "        self.depth = depth\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Describe the model as a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Declare embedding vectors, LSTM cells, and logit layer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Var\n",
    "            self.embedding = tf.Variable(\n",
    "                tf.random_uniform([vocab, dims], -0.02, 0.02))\n",
    "            self.lstm = []\n",
    "            for i in range(depth):\n",
    "                self.lstm.append(LSTM(i, self.dims))\n",
    "            with tf.name_scope('sm'):\n",
    "                self.sm_w = tf.Variable(\n",
    "                    tf.random_uniform([dims, vocab], -0.1, 0.1),\n",
    "                    name='w')\n",
    "                self.sm_b = tf.Variable(\n",
    "                    tf.zeros([vocab]), name='b')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feeds: inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Feeds.\n",
    "            self.words = tf.placeholder(tf.int64)\n",
    "            self.targets = tf.placeholder(tf.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward function\n",
    "  - How to compute from the inputs to the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Define forward.\n",
    "            batch_size = tf.shape(self.words)[:1] \n",
    "            shape = tf.concat(0, [batch_size, [dims]]) \n",
    "            init_zeros = tf.zeros(shape)\n",
    "            h = [init_zeros] * depth\n",
    "            c = [init_zeros] * depth\n",
    "            o = []\n",
    "            \n",
    "            # Unroll LSTMs.\n",
    "            for i in range(steps):\n",
    "                # Get the embedding for words\n",
    "                x = tf.nn.embedding_lookup(\n",
    "                    self.embedding, self.words[:, i])\n",
    "                for j in range(self.depth):\n",
    "                    h[j], c[j] = self.lstm[j].forward(x, h[j], c[j])\n",
    "                    x = h[j]\n",
    "                o.append(x)\n",
    "            outputs = tf.reshape(tf.concat(1, o), [-1, dims])\n",
    "            logits = tf.matmul(outputs, self.sm_w) + self.sm_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss: the optimization goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Compute the loss.\n",
    "            costs = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits, tf.reshape(self.targets, [-1]))\n",
    "            self.loss = tf.reduce_mean(costs)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizer: how to minimize the loss\n",
    "  - Clip gradients before applying to parameters\n",
    "  - Use `tf.train.GradientDescentOptimizer` to reduce some boiler plate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Define gradients, optimizer.\n",
    "            self.global_step = tf.Variable(\n",
    "                0, trainable=False, name='global_step')\n",
    "            vars = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.loss, vars)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            self.train_op = optimizer.apply_gradients(\n",
    "                zip(grads, vars), global_step=self.global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training support\n",
    "  - Summary: pretty plots over time\n",
    "  - Saver: checkpoint trainig state\n",
    "  - Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "            # Summary\n",
    "            tf.scalar_summary('loss', self.loss)\n",
    "            self.summary = tf.merge_summary(\n",
    "                tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "            \n",
    "            # Saver\n",
    "            self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "            # Init\n",
    "            init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "           # Inference\n",
    "            self.in_h = []\n",
    "            self.in_c = []\n",
    "            self.out_h = []\n",
    "            self.out_c = []\n",
    "            x = tf.nn.embedding_lookup(self.embedding, self.words)\n",
    "            for i in range(self.depth):\n",
    "                h = tf.placeholder(tf.float32)\n",
    "                c = tf.placeholder(tf.float32)\n",
    "                self.in_h.append(h)\n",
    "                self.in_c.append(c)\n",
    "                h, c = self.lstm[i].forward(x, h, c)\n",
    "                self.out_h.append(h)\n",
    "                self.out_c.append(c)\n",
    "                x = h\n",
    "            logits = tf.matmul(x, self.sm_w) + self.sm_b\n",
    "            self.preds = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Session\n",
    "  - Connect to TensorFlow runtime\n",
    "  - Initialize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(init)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training\n",
    "  - Iterative algorithm\n",
    "  - Periodic checkpoint and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "    def train(self, data, logdir, total_steps):\n",
    "        swriter = tf.train.SummaryWriter(logdir)\n",
    "        \n",
    "        # Recover.\n",
    "        load(tf.train.latest_checkpoint(logdir))\n",
    "\n",
    "        steps = self.sess.run(self.global_step)\n",
    "        while steps < total_steps:\n",
    "            if steps % 1000 == 0:\n",
    "                self.saver.save(\n",
    "                    self.sess, logdir + '/lm_params', global_step=steps)\n",
    "                w, t = data.get_batch()\n",
    "            if steps % 100 == 0:\n",
    "                loss, summary = self.sess.run(\n",
    "                    [self.loss, self.summary],\n",
    "                    feed_dict={self.words: w, self.targets: t})\n",
    "                swriter.add_summary(summary, steps)\n",
    "                swriter.flush()\n",
    "                print('step %d: %.3f', steps, loss)\n",
    "            else:\n",
    "                self.sess.run(\n",
    "                    self.train_op,\n",
    "                    feed_dict={self.words: w, self.targets: t})\n",
    "            steps += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def load(self, checkpoint):\n",
    "        if checkpoint is not None:\n",
    "            print('restore %s', latest)\n",
    "            self.saver.restore(self.sess, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "  - Extend a sentence with a few more characters.\n",
    "  - E.g., 明月几时有，... ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def inference(self, data, prefix, num):\n",
    "        feeds = {}\n",
    "        zeros = np.zeros([1, self.dims])\n",
    "        for i in range(self.depth):\n",
    "            feeds[self.in_h[i]] = zeros\n",
    "            feeds[self.in_c[i]] = zeros\n",
    "        feeds[self.words] = np.zeros([1], dtype=np.int64)\n",
    "        output = []\n",
    "        for i in range(len(prefix) + num):\n",
    "            if i < len(prefix):\n",
    "                id = data.word_to_id[prefix[i]]\n",
    "            else:\n",
    "                id = int(np.argwhere(np.cumsum(probs) >= np.random.rand())[0])\n",
    "                output.append(id)\n",
    "            feeds[self.words][0] = id\n",
    "            vals = self.sess.run(self.out_h + self.out_c + [self.preds],\n",
    "                                 feed_dict=feeds)\n",
    "            for i in range(self.depth):\n",
    "                feeds[self.in_h[i]] = vals[i]\n",
    "                feeds[self.in_c[i]] = vals[self.depth + i]\n",
    "            probs = np.reshape(vals[-1], [-1])\n",
    "        return prefix + ''.join([data.id_to_word[x] for x in output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Instantiate the data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique chars:  7957\n",
      "Number of training chars:  4888154\n"
     ]
    }
   ],
   "source": [
    "# Main driver.\n",
    "corpus = './data/poem.txt'\n",
    "batch = 32\n",
    "steps = 20\n",
    "data = TrainData(corpus, batch, steps)\n",
    "\n",
    "dims = 256\n",
    "vocab = data.vocab\n",
    "depth = 4\n",
    "steps = 20\n",
    "lr = 0.5\n",
    "model = Model(dims, data.vocab, depth, steps, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We are off to the races!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore  ./lm_params-0\n",
      "step 0: 8.982\n"
     ]
    }
   ],
   "source": [
    "model.train(data, './', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generate sentences\n",
    "  - Start off with few words\n",
    "  - Sample from the probability distribution to get the next word\n",
    "  - Remember to feed the cell state back into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国破山河在，骘吴軏萎鹥馓窋躁寇宣漝诎娜方襕稷斗薍\n",
      "慈母手中线，膈桫椰纑篡得帆烟彳纨纴匈莦思殳彧笙駏\n",
      "一览众山小，榖恙＊拲藭掊髓许滮步牣皿蘅暍蝮姿祼骢\n",
      "明月几时有，阎鞠綍岁殊番擂从羝弯水缣禬彣蛊搬攘狁\n"
     ]
    }
   ],
   "source": [
    "model.load('./lm_params-0')\n",
    "print(model.inference(data, '国破山河在，', 18))\n",
    "print(model.inference(data, '慈母手中线，', 18))\n",
    "print(model.inference(data, '一览众山小，', 18))\n",
    "print(model.inference(data, '明月几时有，', 18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Result\n",
    "* Takes time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国破山河在，门门道路遥。神山犹不见，旧去有人归。\n",
      "慈母手中线，防我巴乡陪。_归去俱结老，栖栖仍及真\n",
      "一览众山小，迫之未归忘。中有秋不尽，殷勤深所随。\n",
      "明月几时有，_楚乡烟霞隔处看。江影漫恓飏记夏，乡\n"
     ]
    }
   ],
   "source": [
    "model.load('./data/lm_params-658000')\n",
    "print(model.inference(data, '国破山河在，', 18))\n",
    "print(model.inference(data, '慈母手中线，', 18))\n",
    "print(model.inference(data, '一览众山小，', 18))\n",
    "print(model.inference(data, '明月几时有，', 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "水光潋滟晴方好，漠漠银花水片飞。_自是冥深萝树间，渔翁于尔亦依然。\n"
     ]
    }
   ],
   "source": [
    "print(model.inference(data, '水光潋滟晴方好，', 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前不见古人，_独来好理人之常。饭酒炊橙胜种畦，病\n"
     ]
    }
   ],
   "source": [
    "print(model.inference(data, '前不见古人，', 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "故人西辞黄鹤楼，上林始觉皆含春。\n"
     ]
    }
   ],
   "source": [
    "print(model.inference(data, '故人西辞黄鹤楼，', 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "郎骑竹马来，_总随高卧跨天鸡。时危或实知吾吝，庸\n"
     ]
    }
   ],
   "source": [
    "print(model.inference(data, '郎骑竹马来，', 18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "* Have fun with sentence generation!\n",
    "* Train a bigger model.\n",
    "* Different data set.\n",
    "  - wiki: http://mattmahoney.net/dc/text8.zip\n",
    "* Train faster\n",
    "  - Change LSTM forward to do one matrix multiplication\n",
    "  - Try other optimizer. E.g., AdamOptimizer"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
